{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import mediapipe.python.solutions\n",
    "import string\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_hands_detect(image, model): #in this case we pass in instance of Hands class object in model args\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Mediapipe accepts only RGB, so gotta convert native color BGR to RGB\n",
    "    image.flags.writeable = False # saves memory, image no longer writeable for time being\n",
    "    results = model.process(image) # image comes from OpenCV frame\n",
    "    #print(results.multi_hand_landmarks)\n",
    "    #print(type(results.multi_hand_landmarks))\n",
    "    #print(len(results.multi_hand_landmarks) if type(results.multi_hand_landmarks) == list else print(0))\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # afterwards, we convert from RGB to BGR back\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_hand_landmarks(image, results):\n",
    "    if type(results.multi_hand_landmarks) == list:\n",
    "        mp_drawing.draw_landmarks(image, results.multi_hand_landmarks[0], mp_hands.HAND_CONNECTIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_hand_landmarks(image, results): # same as rendering hand land marks but with extra styling\n",
    "    if type(results.multi_hand_landmarks) == list:\n",
    "        mp_drawing.draw_landmarks(image, \n",
    "                                  results.multi_hand_landmarks[0], \n",
    "                                  mp_hands.HAND_CONNECTIONS,\n",
    "                                  # arranged in BGR because remember we converted the image back from RGB to BGR\n",
    "                                  mp_drawing.DrawingSpec(color=(86,255,255), thickness=2, circle_radius=4),\n",
    "                                  mp_drawing.DrawingSpec(color=(170,86,255), thickness=2, circle_radius=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Begin use mediapipe\n",
    "with mp_hands.Hands(max_num_hands = 1, min_detection_confidence=0.3, min_tracking_confidence=0.3) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mp_hands_detect(frame, hands)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        style_hand_landmarks(image, results)\n",
    "\n",
    "        # Display to screen\n",
    "        cv2.imshow('FSL Alphabet Detector Cam Test', image)\n",
    "        # if q has been pressed for about 10 frames, quit\n",
    "        if (cv2.waitKey(10) & 0xFF == ord('q')):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "landmark {\n",
       "  x: 0.2706824541091919\n",
       "  y: 0.7579841613769531\n",
       "  z: 8.316048933920683e-07\n",
       "}\n",
       "landmark {\n",
       "  x: 0.3643667995929718\n",
       "  y: 0.7712401747703552\n",
       "  z: -0.04906079173088074\n",
       "}\n",
       "landmark {\n",
       "  x: 0.44633594155311584\n",
       "  y: 0.7371939420700073\n",
       "  z: -0.08079937100410461\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5084257125854492\n",
       "  y: 0.7030449509620667\n",
       "  z: -0.10914406925439835\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5715792775154114\n",
       "  y: 0.6892408132553101\n",
       "  z: -0.14070983231067657\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4326872229576111\n",
       "  y: 0.5544135570526123\n",
       "  z: -0.08119431883096695\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4992033541202545\n",
       "  y: 0.4523717164993286\n",
       "  z: -0.12834380567073822\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5438634157180786\n",
       "  y: 0.39030057191848755\n",
       "  z: -0.1607019305229187\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5823380351066589\n",
       "  y: 0.3344220221042633\n",
       "  z: -0.18320265412330627\n",
       "}\n",
       "landmark {\n",
       "  x: 0.37588247656822205\n",
       "  y: 0.5195726752281189\n",
       "  z: -0.08583048731088638\n",
       "}\n",
       "landmark {\n",
       "  x: 0.41871264576911926\n",
       "  y: 0.4030507802963257\n",
       "  z: -0.12717150151729584\n",
       "}\n",
       "landmark {\n",
       "  x: 0.45174598693847656\n",
       "  y: 0.33059507608413696\n",
       "  z: -0.15519951283931732\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4797542691230774\n",
       "  y: 0.26496267318725586\n",
       "  z: -0.17614245414733887\n",
       "}\n",
       "landmark {\n",
       "  x: 0.31136131286621094\n",
       "  y: 0.522205114364624\n",
       "  z: -0.094451904296875\n",
       "}\n",
       "landmark {\n",
       "  x: 0.3305722177028656\n",
       "  y: 0.4096272587776184\n",
       "  z: -0.13603588938713074\n",
       "}\n",
       "landmark {\n",
       "  x: 0.3447119891643524\n",
       "  y: 0.3380568027496338\n",
       "  z: -0.16113635897636414\n",
       "}\n",
       "landmark {\n",
       "  x: 0.36051493883132935\n",
       "  y: 0.2717556953430176\n",
       "  z: -0.1787840574979782\n",
       "}\n",
       "landmark {\n",
       "  x: 0.24609872698783875\n",
       "  y: 0.5531744360923767\n",
       "  z: -0.10655388981103897\n",
       "}\n",
       "landmark {\n",
       "  x: 0.2368498146533966\n",
       "  y: 0.4700453281402588\n",
       "  z: -0.14781427383422852\n",
       "}\n",
       "landmark {\n",
       "  x: 0.2287314236164093\n",
       "  y: 0.4216414988040924\n",
       "  z: -0.16576455533504486\n",
       "}\n",
       "landmark {\n",
       "  x: 0.222998708486557\n",
       "  y: 0.3739941120147705\n",
       "  z: -0.1767488420009613\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results.multi_hand_landmarks[0].landmark)\n",
    "results.multi_hand_landmarks[0]\n",
    "# np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks.landmark]).flatten() # get all xyz values then combine them into one array using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    # Get landmark points from active frame\n",
    "    # if there is none detected then output zeros\n",
    "    single_hand = np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[0].landmark]).flatten() if results.multi_hand_landmarks else np.zeros(21*3)\n",
    "    return single_hand\n",
    "#len(results.multi_hand_landmarks[0].landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define directories\n",
    "working_dir = os.path.join(os.path.abspath(''), 'Datasets')\n",
    "training_dir = os.path.join(working_dir, 'train')\n",
    "testing_dir = os.path.join(working_dir, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(os.path.abspath(''), 'MP_Data')\n",
    "# stuff we try to detect\n",
    "alphabets = np.array(list(string.ascii_uppercase))\n",
    "\n",
    "# 30 videos \n",
    "no_sequences = 30\n",
    "\n",
    "# videos are 30 frames of length\n",
    "sequence_length = 30\n",
    "\n",
    "# A\n",
    "## 0\n",
    "## 1\n",
    "## ...29\n",
    "# B\n",
    "## 0\n",
    "## 1\n",
    "## ...29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alphabet in alphabets:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, alphabet, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# This cell is responsible for collecting training data from images, converted into numpy array containing landmark data\n",
    "with mp_hands.Hands(max_num_hands = 1, min_detection_confidence=0.5) as hands:\n",
    "    for alphabet in alphabets:\n",
    "        for sequence in range(no_sequences):\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mp_hands_detect(frame, hands)\n",
    "                \n",
    "                # Draw landmarks\n",
    "                style_hand_landmarks(image, results)\n",
    "\n",
    "                # collection time\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'STARTING COLLECTION, Press Y to start', (120,200),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, f'Collecting frames for alphabet {alphabet} Video Number {sequence}', (15,12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    key = cv2.waitKey(0)\n",
    "                    if key == ord('y'):\n",
    "                        cv2.waitKey(1)\n",
    "                else:\n",
    "                    cv2.putText(image, f'Collecting frames for alphabet {alphabet} Video Number {sequence}', (15,12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                # export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, alphabet, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                if (cv2.waitKey(10) & 0xFF == ord('q')):\n",
    "                    break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.16032350e-01,  8.24139833e-01, -6.43885016e-07,  4.04884994e-01,\n",
       "        7.96038628e-01, -2.23944895e-02,  4.64068592e-01,  6.75332665e-01,\n",
       "       -2.29087770e-02,  4.77242380e-01,  5.65811634e-01, -2.29458753e-02,\n",
       "        5.00718355e-01,  5.00734150e-01, -1.48792621e-02,  4.29109752e-01,\n",
       "        5.80771983e-01, -3.11114057e-03,  4.47404563e-01,  5.10573030e-01,\n",
       "       -3.82338762e-02,  4.32087481e-01,  6.05695963e-01, -4.62270044e-02,\n",
       "        4.17190135e-01,  6.45211637e-01, -4.43341881e-02,  3.78706694e-01,\n",
       "        5.80517232e-01, -4.32268996e-03,  3.99524599e-01,  5.18978179e-01,\n",
       "       -4.37962227e-02,  3.88011128e-01,  6.30020082e-01, -4.24944721e-02,\n",
       "        3.74779403e-01,  6.56488597e-01, -3.03943809e-02,  3.27853173e-01,\n",
       "        5.89104116e-01, -1.26247751e-02,  3.48750234e-01,  5.36244929e-01,\n",
       "       -5.90156801e-02,  3.43426168e-01,  6.44371092e-01, -4.07166407e-02,\n",
       "        3.31948489e-01,  6.68388546e-01, -1.52929919e-02,  2.72800058e-01,\n",
       "        6.02422595e-01, -2.18734313e-02,  3.00541282e-01,  5.63179791e-01,\n",
       "       -4.81291562e-02,  3.02131087e-01,  6.38320625e-01, -3.14301997e-02,\n",
       "        2.90698230e-01,  6.59398079e-01, -1.14118373e-02])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test read NPY file\n",
    "\n",
    "test_dir = os.path.join(os.path.abspath(''), 'MP_Data\\\\A\\\\0')\n",
    "npy_file = os.path.join(test_dir, '4.npy')\n",
    "data = np.load(npy_file)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing, features, and labeling\n",
    "from sklearn.model_selection import train_test_split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FSL-Alphabet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
